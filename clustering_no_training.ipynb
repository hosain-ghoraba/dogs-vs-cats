{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> Global Variables & Paths <h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------------------------- Global Variables & Paths\n",
        "# region\n",
        "# @title Global Variables & Paths { display-mode: \"form\" }\n",
        "import psutil\n",
        "print(f\"RAM usage at the start of the notebook is: {psutil.virtual_memory().used/1e9} GB\")\n",
        "import os\n",
        "\n",
        "# --------------- vscode\n",
        "VS_CODE_RAW_DATA_SET_PATH = r\"C:\\Users\\Dell\\Desktop\\Bachelor\\datasets\\26 letters\\2500\\2500 balanced\" # modify per data set\n",
        "VS_CODE_CLEAN_DATA_SET_PATH = '../image_classification/data/clean_data'\n",
        "\n",
        "# --------------- kaggle\n",
        "KAGGLE_RAW_DATA_SET_PATH = '/kaggle/input/position-categorized-handwritten-arabic-letters' # put (dataset name immediately after /input), and any extra folders modify per data set\n",
        "KAGGLE_CLEAN_DATA_SET_PATH = '/kaggle/working/clean_data'\n",
        "\n",
        "\n",
        "# --------------- colab\n",
        "KAGGLE_CRED_PATH = '/content/gdrive/MyDrive/Bachelor/kaggle'\n",
        "KAGGLE_DATA_SET_NAME = 'mahmoudreda55/arabic-letters-numbers-ocr'  # username/dataset name..t modify per data set\n",
        "COLAB_RAW_DATA_SET_PATH = '/content/kaggle_data/Dataset' # put (dataset_name) after (kaggle_data) , then put any extra folders... modify the part after (kaggle_data) per data set\n",
        "COLAB_CLAEN_DATA_SET_PATH = '/content/clean_data'\n",
        "\n",
        "# ----------------- global variables\n",
        "EPOCHS = 100\n",
        "DATA_PERCENTAGE_TO_USE = 100\n",
        "image_size = 64\n",
        "USE_RAW_DATA = True\n",
        "RENAME_CLEAN_DATA = False\n",
        "# ----------------- for clustering\n",
        "CLUSTERS_COUNT = 10\n",
        "CLUSTERS_IMAGES_DEST = os.path.join(os.getcwd(), 'clusters_images')\n",
        "ESTIMATE_OPTIMAL_NO_OF_CLUSTERS = False\n",
        "\n",
        "# print(\"img size: \", image_size)\n",
        "print(\"clusters count: \", CLUSTERS_COUNT)\n",
        "print('Global Variables & Paths are set successfully.')\n",
        "# endregion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> time calculation function <h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------------------------- time calculation function\n",
        "# region\n",
        "# @title time calculation function { display-mode: \"form\" }\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "global_start_time = time.time() # to calculate time taken for the whole notebook to run\n",
        "def calculate_and_print_time(start_time , section_name):\n",
        "    end_time = time.time()\n",
        "    time_taken = end_time - start_time\n",
        "    hours = int(time_taken // 3600)\n",
        "    minutes = int((time_taken % 3600) // 60)\n",
        "    seconds = int((time_taken % 3600) % 60)\n",
        "    print(f'\\n{section_name} done in : {hours} h, {minutes} m, {seconds} s')\n",
        "\n",
        "print('time calculation function is set successfully.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> install libraries <h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------------------------- install libraries\n",
        "# region\n",
        "# @title install libraries { display-mode: \"form\" }\n",
        "\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "import pkg_resources\n",
        "REQUIRED_PACKAGES = ['seaborn', 'tensorflow','natsort',\"psutil\",\"pympler\",\"arabic-reshaper\",\"python-bidi\"]\n",
        "\n",
        "for package in REQUIRED_PACKAGES:\n",
        "    try:\n",
        "        dist = pkg_resources.get_distribution(package)\n",
        "        print('{} ({}) is installed'.format(dist.key, dist.version))\n",
        "    except pkg_resources.DistributionNotFound:\n",
        "        print('{} is NOT installed'.format(package))\n",
        "        %pip install {package} \n",
        "# uninstall the standalone keras package to force using the one installed with tensorflow\n",
        "# %pip uninstall keras -y\n",
        "\n",
        "\n",
        "calculate_and_print_time(start_time, 'installing required packages')\n",
        "# print(\"installing required packages done\")\n",
        "# endregion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> imports <h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------------------------- imports\n",
        "# region\n",
        "# @title imports { display-mode: \"form\" }\n",
        "start_time = time.time()\n",
        "\n",
        "import uuid\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import warnings\n",
        "import shutil\n",
        "import zipfile\n",
        "from natsort import natsorted\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "# import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense,MaxPooling2D,Dropout,Flatten,BatchNormalization,Conv2D\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping\n",
        "from keras.preprocessing import image\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import psutil\n",
        "from pympler import asizeof\n",
        "import gc\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "import io\n",
        "import contextlib\n",
        "\n",
        "\n",
        "calculate_and_print_time(start_time, 'imports')\n",
        "# print(\"imports done\")\n",
        "\n",
        "# endregion\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> detect Hardware <h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------------------------- detect Hardware\n",
        "# region\n",
        "# @title detect Hardware {display-mode: \"form\"}\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    print(\"Running on TPU\")\n",
        "    print('TPU details: ', tpu.cluster_spec().as_dict())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if(gpus):\n",
        "        print(\"Running on \", len(gpus), \" GPU(s) \")\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        strategy = tf.distribute.MirroredStrategy(devices=[\"GPU:{}\".format(i) for i in range(len(gpus))])\n",
        "    else:        \n",
        "        print(\"Running on CPU\")\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
        "calculate_and_print_time(start_time,'detecting hardware')\n",
        "# extra\n",
        "# endregion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> functions <h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------------------------- functions\n",
        "# region\n",
        "# @title functions {display-mode: \"form\"}\n",
        "\n",
        "\n",
        "import dis\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "raw_data_path = None\n",
        "clean_data_path = None\n",
        "\n",
        "def save_clusters_images_paths(data):\n",
        "    clusters_summary = {}\n",
        "    clusters_images_paths = {}\n",
        "    for cluster in sorted(data['cluster'].unique()):\n",
        "        cluster_key = f'cluster{int(cluster)}'\n",
        "        cluster_data = data[data['cluster'] == cluster]\n",
        "        cluster_classes = cluster_data['label'].unique().tolist()\n",
        "\n",
        "        clusters_summary[cluster_key] = {\n",
        "            'num_classes': len(cluster_classes),\n",
        "            'classes': cluster_classes,\n",
        "        }\n",
        "        clusters_images_paths[cluster_key] = cluster_data['filename'].tolist()\n",
        "\n",
        "    json_data = json.dumps({'clusters_summary': clusters_summary,'clusters_images_paths': clusters_images_paths},ensure_ascii=False,indent=4)\n",
        "    created_json_file_name = f'clusters_info_dataset-perc{DATA_PERCENTAGE_TO_USE}_clusters{CLUSTERS_COUNT}'\n",
        "    dest_path = os.path.join(os.getcwd(), f'{created_json_file_name}.json')\n",
        "    with open(dest_path, 'w',encoding='utf-8') as f:\n",
        "        f.write(json_data)\n",
        "\n",
        "# Function to transform an image into a feature vector\n",
        "def image_to_feature_vector(img_path,model):\n",
        "    img = image.load_img(img_path, target_size=(224, 224)) # 224 x 224 is the size of images that VGG16 previously trained on\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    features = model.predict(x)\n",
        "    return features.reshape(-1)\n",
        "\n",
        "\n",
        "def delete_from_RAM(variable_names):\n",
        "    print_current_ram_usage(f\"before deleting : {variable_names}\")\n",
        "    for var_name in variable_names:\n",
        "        if var_name in globals():\n",
        "            var_size_in_GB = asizeof.asizeof(globals()[var_name]) / 1e9\n",
        "            print(f\"variable {var_name} with size {var_size_in_GB} GB is in globals , and will be deleted\")\n",
        "            del globals()[var_name]\n",
        "        elif var_name in locals():\n",
        "            var_size_in_GB = asizeof.asizeof(locals()[var_name]) / 1e9\n",
        "            print(f\"variable {var_name} with size {var_size_in_GB} GB is in locals , and will be deleted\")\n",
        "        print()\n",
        "    gc.collect()\n",
        "    print(f\"Deleted {variable_names} from RAM\")\n",
        "    print_current_ram_usage(f\"after deleting : {variable_names}\")\n",
        "\n",
        "\n",
        "def print_current_ram_usage(message = \"\"):\n",
        "    print(f\"RAM usage {message}: {psutil.virtual_memory().used / 1e9} GB\")\n",
        "def get_environment():\n",
        "    if 'COLAB_GPU' in os.environ:\n",
        "        return 'Google Colab'\n",
        "    elif 'KAGGLE_URL_BASE' in os.environ:\n",
        "        return 'Kaggle'\n",
        "    elif 'VSCODE_PID' in os.environ:\n",
        "        return 'VS Code'\n",
        "    else:\n",
        "        return 'Unknown environment'\n",
        "\n",
        "def do_colab_staff():\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "\n",
        "    # storing kaggle credentials\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = KAGGLE_CRED_PATH\n",
        "\n",
        "    !kaggle datasets download -d {KAGGLE_DATA_SET_NAME}\n",
        "    print(\"downloaded dataset\" ,KAGGLE_DATA_SET_NAME )\n",
        "    \n",
        "    \n",
        "    ! mkdir kaggle_data\n",
        "    downloaded_zip_name = f\"{KAGGLE_DATA_SET_NAME.split('/')[-1]}.zip\" # the !kaggle datasets download command will download the zip file with the same name as the dataset name\n",
        "    extract_folder_path = 'kaggle_data'\n",
        "    extract_zip(downloaded_zip_name, extract_folder_path)\n",
        "\n",
        "    global raw_data_path\n",
        "    raw_data_path = COLAB_RAW_DATA_SET_PATH\n",
        "    global clean_data_path\n",
        "    clean_data_path = COLAB_CLAEN_DATA_SET_PATH\n",
        "\n",
        "\n",
        "def do_kaggle_staff():\n",
        "    global raw_data_path\n",
        "    raw_data_path = KAGGLE_RAW_DATA_SET_PATH\n",
        "    global clean_data_path\n",
        "    clean_data_path = KAGGLE_CLEAN_DATA_SET_PATH\n",
        "    \n",
        "def do_vscode_staff():\n",
        "    global raw_data_path\n",
        "    raw_data_path = VS_CODE_RAW_DATA_SET_PATH\n",
        "    global clean_data_path\n",
        "    clean_data_path = VS_CODE_CLEAN_DATA_SET_PATH\n",
        "\n",
        "def do_unknown_environment_staff():\n",
        "    print(\"This is an unknown environment, please enter the path to the data set folder:\")\n",
        "    global raw_data_path\n",
        "    raw_data_path = input()     \n",
        "\n",
        "def image_is_ok(image_path):\n",
        "    with warnings.catch_warnings(record=True) as w:\n",
        "        try:\n",
        "            img = Image.open(image_path)\n",
        "            img.verify()\n",
        "            if len(w) > 0:  # if any warnings were issued\n",
        "                return False\n",
        "            return True\n",
        "        except (IOError, SyntaxError):\n",
        "            return False\n",
        "\n",
        "def copy_clean_files(dirty_dataset_path, destination_folder_path):\n",
        "    print(\"copying clean files...\")\n",
        "    # If the destination directory already exists, remove it and all its contents\n",
        "    if os.path.exists(destination_folder_path):\n",
        "        print(f\"removing {destination_folder_path} directory to create a new one...\")\n",
        "        shutil.rmtree(destination_folder_path)\n",
        "        print(f\"removed {destination_folder_path} directory to create a new one...\")\n",
        "    os.makedirs(destination_folder_path)\n",
        "    \n",
        "    # Get the total number of files for the progress bar\n",
        "    total_files = sum([len(files) for r, d, files in os.walk(dirty_dataset_path)])\n",
        "    \n",
        "    progress_bar = tqdm(total=total_files, desc=\"Copying files\", unit=\"file\")\n",
        "    \n",
        "    for root, dirs, files in os.walk(dirty_dataset_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            if image_is_ok(file_path):\n",
        "                new_file_path = os.path.join(destination_folder_path, os.path.relpath(file_path, dirty_dataset_path))\n",
        "                os.makedirs(os.path.dirname(new_file_path) , exist_ok=True)\n",
        "                shutil.copyfile(file_path, new_file_path)\n",
        "            else:\n",
        "                print(f\"file: {os.path.relpath(file_path, dirty_dataset_path)} is corrupted & skipped from dataset while copying \")\n",
        "            \n",
        "            progress_bar.update(1)\n",
        "    \n",
        "    progress_bar.close()\n",
        "    print(f\"copied files from {dirty_dataset_path} to {destination_folder_path} successfully\") \n",
        "\n",
        "def rename_files(data_set_path):\n",
        "    all_entities_names = natsorted(os.listdir(data_set_path))\n",
        "    print(\"giving temporary unique names...\")\n",
        "    for entity_name in all_entities_names:\n",
        "        entity_path = os.path.join(data_set_path, entity_name)\n",
        "        for filename in os.listdir(entity_path):\n",
        "            temp_filename = str(uuid.uuid4()) + \".jpg\"  # generate a unique filename\n",
        "            source = os.path.join(entity_path, filename)\n",
        "            destination = os.path.join(entity_path, temp_filename)\n",
        "            os.rename(source, destination)\n",
        "    # ----------------------------------------------------------------------------------------------------\n",
        "    print(\"renaming...\")\n",
        "    # then rename every file in every folder in the given path\n",
        "\n",
        "    for entity_name in all_entities_names:\n",
        "        entity_path = os.path.join(data_set_path, entity_name)\n",
        "        i = 1\n",
        "        for filename in os.listdir(entity_path):\n",
        "            entity_name = os.fsdecode(entity_name.lower())\n",
        "            new_filename = entity_name + '.' + str(i) + \".jpg\"\n",
        "            source = os.path.join(entity_path, filename)\n",
        "            destination = os.path.join(entity_path, new_filename)\n",
        "            os.rename(source, destination)\n",
        "            i += 1\n",
        "    print(\"done renaming !\")\n",
        "\n",
        "\n",
        "def extract_zip(source_path, destination_path):\n",
        "    # delete the destination folder if it exists\n",
        "    if os.path.exists(destination_path):\n",
        "        print('Deleting the existing destination folder...')\n",
        "        shutil.rmtree(destination_path)\n",
        "\n",
        "    print(\"Extracting files...\") \n",
        "    with zipfile.ZipFile(source_path, 'r') as zip_ref:\n",
        "        files = zip_ref.infolist()\n",
        "        for file in tqdm(files, desc=\"Extracting files\", unit=\"file\"):\n",
        "            try:\n",
        "                file.filename = file.filename.encode('cp437').decode('utf-8')  # try 'cp437' encoding first\n",
        "            except UnicodeDecodeError:\n",
        "                file.filename = file.filename.encode('utf-8').decode('utf-8')  # fallback to 'utf-8' if 'cp437' fails\n",
        "            zip_ref.extract(file, path=destination_path)\n",
        "\n",
        "    print(\"Extraction complete.\")\n",
        "\n",
        "\n",
        "calculate_and_print_time(start_time, 'functions')\n",
        "# print(\"functions done\")\n",
        "# endregion\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> doing specific-environment things <h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------------------------- doing specific-environment things\n",
        "# region\n",
        "# @title doing specific-environment things {display-mode: \"form\"}\n",
        "start_time = time.time()\n",
        "\n",
        "# test if the tpu & GPU is available\n",
        "\n",
        "raw_data_path = None    \n",
        "environment_type = get_environment() \n",
        "print(f'Environment: {environment_type}')     \n",
        "\n",
        "print(\"tensowflow version:\", tf.__version__)\n",
        "if environment_type == 'Google Colab':\n",
        "    # do_colab_staff()\n",
        "    print(\"colab staff skipped !,doing kaggle staff instead...\")\n",
        "    do_kaggle_staff()\n",
        "elif environment_type == 'Kaggle':\n",
        "    do_kaggle_staff()\n",
        "elif environment_type == 'VS Code':\n",
        "    do_vscode_staff()\n",
        "else:\n",
        "    do_unknown_environment_staff()\n",
        "\n",
        "print(\"raw data set path:\", raw_data_path)\n",
        "assert os.path.exists(raw_data_path), ' wrong path for data set !' \n",
        "print(\"clean data set path:\", clean_data_path)\n",
        "  \n",
        "calculate_and_print_time(start_time, 'doing specific-environment things')\n",
        "print_current_ram_usage()\n",
        "\n",
        "# endregion\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> data preparation <h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------------------------------------------------- data preparation\n",
        "# region\n",
        "# @title data preparation {display-mode: \"form\"}\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"raw data path:\", raw_data_path)\n",
        "if USE_RAW_DATA:\n",
        "    ready_data_path = raw_data_path\n",
        "    print(\"we will use raw data directly\")\n",
        "else :\n",
        "    copy_clean_files(raw_data_path, clean_data_path)\n",
        "    if RENAME_CLEAN_DATA:\n",
        "        rename_files(clean_data_path)\n",
        "    ready_data_path = clean_data_path        \n",
        "\n",
        "# ----------------------------\n",
        "\n",
        "print(\"we will use \", DATA_PERCENTAGE_TO_USE, \"% of the data\")\n",
        "calculate_and_print_time(start_time, 'data preparation')\n",
        "print_current_ram_usage()\n",
        "\n",
        "# endregion\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1 > 1 - Loading Images in a Dataframe <h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9g1mXLiOBTX"
      },
      "outputs": [],
      "source": [
        "# 1 --------------------------------------------------------- Loading Images in a Dataframe\n",
        "# region\n",
        "# @title 1 - Loading Images in a Dataframe { display-mode: \"form\" }\n",
        "\n",
        "\n",
        "from matplotlib.style import available\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "all_entities_names = natsorted(os.listdir(ready_data_path))\n",
        "print(\"all entities names:\", all_entities_names)\n",
        "filenames = []\n",
        "original_data_set_size = 0\n",
        "feature_extraction_model = VGG16(weights='imagenet', include_top=False)\n",
        "\n",
        "all_feature_vectors = []\n",
        "\n",
        "for entity_name in all_entities_names:\n",
        "    print(f\"loading ({entity_name}) images in the dataframe\")\n",
        "    entity_path = os.path.join(ready_data_path, entity_name)\n",
        "    entity_filenames = [file_name for file_name in os.listdir(entity_path)]\n",
        "    original_data_set_size += len(entity_filenames)\n",
        "    random.shuffle(entity_filenames)\n",
        "    entity_filenames = entity_filenames[:int(len(entity_filenames) * (DATA_PERCENTAGE_TO_USE / 100))]\n",
        "    filenames.extend([os.path.join(entity_name, file_name) for file_name in entity_filenames])\n",
        "\n",
        "    # Initialize feature_vectors array for current entity\n",
        "    feature_vectors = np.empty((len(entity_filenames), 25088))\n",
        "    \n",
        "    print(f\"extracting features from ({entity_name}) images...\")\n",
        "    trash_printings = io.StringIO()\n",
        "    with contextlib.redirect_stdout(trash_printings): # just to suppress the output of the following code\n",
        "        for i, entity_file_name in enumerate(entity_filenames):\n",
        "            img_path = os.path.join(entity_path, entity_file_name)\n",
        "            feature_vectors[i] = image_to_feature_vector(img_path, feature_extraction_model)\n",
        "    all_feature_vectors.append(feature_vectors)\n",
        "\n",
        "# Combine all feature vectors into one array\n",
        "all_feature_vectors = np.concatenate(all_feature_vectors)\n",
        "\n",
        "\n",
        "print_current_ram_usage(\"after making the flattened_images np array\")\n",
        "\n",
        "print(f\"selected data set size = {DATA_PERCENTAGE_TO_USE / 100} * {original_data_set_size} = {len(filenames)}\") \n",
        "file_labels = [x.split(os.sep)[0] for x in filenames] \n",
        "data = pd.DataFrame({\"filename\": filenames, \"label\": file_labels})\n",
        "data = data.iloc[natsorted(data.index.values)] # sort the data frame by the labels using natsort\n",
        "\n",
        "# clustering part ------------------------------------------------------------\n",
        "\n",
        "# -------------------- Elbow Method to find the optimal number of clusters\n",
        "# if ESTIMATE_OPTIMAL_NO_OF_CLUSTERS:\n",
        "#     print(\"Elbow Method to find the optimal number of clusters...\")\n",
        "#     wcss = []\n",
        "#     for i in range(1, 11):  # change the range according to your needs\n",
        "#         kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "#         kmeans.fit(flattened_images)\n",
        "#         wcss.append(kmeans.inertia_)\n",
        "#     plt.plot(range(1, 11), wcss)\n",
        "#     plt.title('Elbow Method')\n",
        "#     plt.xlabel('Number of clusters')\n",
        "#     plt.ylabel('WCSS')\n",
        "#     plt.show()\n",
        "#     print_current_ram_usage(\"after Elbow Method to find the optimal number of clusters\")\n",
        "# -------------------------------------\n",
        "# this model is used to extract features from the images , to cluster them properly\n",
        "\n",
        "# Perform KMeans clustering\n",
        "(\"performing KMeans clustering...\")\n",
        "\n",
        "batch_size = len(all_feature_vectors) // 50\n",
        "print(f\"batch_size = {len(all_feature_vectors)} // 50 = {batch_size}\")\n",
        "\n",
        "# Get the cluster labels for each image\n",
        "labels = MiniBatchKMeans(n_clusters=CLUSTERS_COUNT, random_state=0, batch_size=batch_size).fit(all_feature_vectors).labels_\n",
        "\n",
        "delete_from_RAM([\"all_feature_vectors\"])\n",
        "data['cluster'] = labels\n",
        "\n",
        "\n",
        "calculate_and_print_time(start_time, 'Loading Images in a Dataframe')\n",
        "# endregion\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Vislalizing clusters <h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2 --------------------------------------------------------- Visualizing Clusters\n",
        "# region\n",
        "# @title 2 - Visualizing Clusters { display-mode: \"form\" }\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import arabic_reshaper\n",
        "from bidi.algorithm import get_display\n",
        "\n",
        "plt.rcParams['font.family'] = 'Arial'\n",
        "\n",
        "def display_entities_distribution_in_each_cluster(data, sort_plot=False):\n",
        "    crosstab = pd.crosstab(data['cluster'], data['label'])\n",
        "    all_classes_total_counts = data['label'].value_counts()  # Total count of each entity in the dataset\n",
        "\n",
        "    for cluster_index in crosstab.index:\n",
        "        cluster_data = crosstab.loc[cluster_index]\n",
        "        cluster_size = sum(cluster_data)\n",
        "        cluster_data /= all_classes_total_counts  # Normalize the count of each entity by the total count of that entity in the dataset\n",
        "        # remove the extities which percentages is less than 10% to make the plot more clear , but first get the number of such entities\n",
        "        skipping_threshhold = 1 / CLUSTERS_COUNT\n",
        "        num_of_skipped_entities = len(cluster_data[cluster_data <= skipping_threshhold])\n",
        "        cluster_data = cluster_data[cluster_data > skipping_threshhold]\n",
        "        if sort_plot:\n",
        "            cluster_data = cluster_data.sort_values(ascending=False)\n",
        "        ax = cluster_data.plot(kind='bar', color='blue', figsize=(20,5))  # Increase figure size and decrease bar width\n",
        "        title_text = f'{\"sorted \" if sort_plot else \" \"}classes distribution in cluster {cluster_index} which has a total of : {cluster_size} images'\n",
        "        title_text += f'\\n Note: y axis represents the percentage of current class images count in current cluster to the total images count of that class in the whole dataest '\n",
        "        # add the number of skipped entities to the title\n",
        "        title_text += f'\\n Note: {num_of_skipped_entities} entities are skipped from the plot because their percentage is less than {skipping_threshhold * 100}%'\n",
        "        plt.title(title_text, fontsize=14)\n",
        "        plt.xlabel('classes')\n",
        "        plt.ylabel('percentage of all class images')\n",
        "        \n",
        "        labels = [get_display(arabic_reshaper.reshape(label.get_text())) for label in ax.get_xticklabels()]\n",
        "        ax.set_xticklabels(labels)\n",
        "        # only rotate if number of classes is less than or equal to 32\n",
        "        rotation_threshold = 32\n",
        "        if len(labels) <= rotation_threshold:\n",
        "            plt.xticks(rotation=45)\n",
        "        plt.show()\n",
        "def visualize_clusters(data):\n",
        "    crosstab = pd.crosstab(data['cluster'], data['label'])\n",
        "    # Calculate the total number of images in each cluster\n",
        "    total_images_per_cluster = crosstab.sum(axis=1)\n",
        "\n",
        "    # pie chart ----------------------------------------\n",
        "    plt.figure(figsize=(20, 7))\n",
        "    chart_lables = ['Cluster ' + str(i) for i in total_images_per_cluster.index]\n",
        "    plt.pie(total_images_per_cluster.values, labels=chart_lables, autopct='%1.1f%%')\n",
        "    plt_title = f'Clusters sizes distribution \\n Dataset size: {len(data)} images'\n",
        "    plt_title += f'\\n Note: each percentage represents the size of the cluster to the total size of the dataset'\n",
        "    plt.title(plt_title, fontsize=14)\n",
        "    plt.show()\n",
        "    # --------------------------------------------------\n",
        "    # For each cluster, plot a histogram of the count of every entity\n",
        "    # display_entities_distribution_in_each_cluster(data)\n",
        "    display_entities_distribution_in_each_cluster(data,sort_plot=True)\n",
        "\n",
        "visualize_clusters(data)\n",
        "save_clusters_images_paths(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Total Execution Time <h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 15 --------------------------------------------------------- Execution Time\n",
        "# region\n",
        "# @title 15 - Execution time  { display-mode: \"form\" }\n",
        "\n",
        "calculate_and_print_time(global_start_time , 'Whole notebook execution time')\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
