1 - brough image classification model for dogs and cats from kaggle, modified it to work for more than 2 classes (26 letters)

2 - trained on both 2500 & 1800 datasets , with different image sizes from 16 to 300 , to discovr that the size 256 is the best for 2500 dataset (99% accracy), and the size 128 is best for 1800 dataset (84% accracy) , here are the table : 

				1800,16/  1800 ,32/  1800 ,64/  1800 ,128/  1800 ,256/  1800 ,300/  2500 ,16/  2500 ,32/  2500 ,64/  2500 ,128/  2500 ,256/  2500 ,300/

sample-1800			0.174000  0.400000   0.475000   0.833000  0.058000    0.040000	   0.069000   0.080000	 0.036000   0.040000	0.056000    0.049000
sample-2500			0.036000  0.042000   0.037000   0.024000  0.060000    0.036000	   0.929000   0.988000	 0.996000   0.979000	0.990000    0.975000
clean-2500-letters-only		0.036000  0.043000   0.036000	0.027000  0.058000    0.036000	   0.933000   0.984000	 0.998000   0.980000	0.986000    0.975000
clean-1800-letters-only		0.156000  0.404000   0.465000	0.842000  0.060000    0.040000	   0.067000   0.073000	 0.040000   0.041000	0.043000    0.049000
 
-- tried to make a cluster of letters baa,taa,thaa for the 1800 dataset, to see if we can improve the 84% accuracy , and indeed when traied a model on 1800 dataset on baa,taa,thaa only, the classification accuracy of the model improved by about 20% for each letter (compared to the accuracy of the model for each letter when trained on the all 1800 dataset)

3 - tested each model on 1800 & 2500 datasets, accuracy for anymodel is very bad (less than 1%) on datasets other than the trained dataset

4 - tried to train the 2500_128 model on only 30% of the dataset , to see if the huge number of samples is the cause of this overfitting, and tested it on 1800&2500 datasets again, but got same results (very good accuracy of 98% for the 2500 dataset) , but (less than 1%) for the 1800 dataset

5 - merged the 2 datasets (1800 & 2500) and trained the model on them again (with image sizes set to 128 & 256) , and got about 88% accuracy for 128 images size , and got a similar accracy (90%) for the 256 images size

6 - tested the models 4000 128 and 4000 256 on the 2 datasets separately (1800 and 2500) and here are the accuracies :

 				4000 ,128_model		4000 ,256_model
clean-2500-letters-only		0.992000		0.988000
clean-1800-letters-only		0.783000		0.843000

7 - tried to train the 1800 128 dataset (but with clustering this time), with k = 4 , but I ran out of RAM memory , so I tried to manually free some unused variables to free the memory at some points, but it did not work because I could not force the python memory manager to delete the variables from the memory, and now I am stuck !!





