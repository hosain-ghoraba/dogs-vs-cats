{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Choose an image to test the model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select an image to classify...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(model_info, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     59\u001b[0m     img_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(json\u001b[38;5;241m.\u001b[39mload(f)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 60\u001b[0m current_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrained_data_size\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ,\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(img_size)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mloading model that was trained on dataset : \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(trained_data_size) , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, with image size : \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(img_size) , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(model_path)\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tkinter import filedialog\n",
    "from tkinter import Tk\n",
    "import os\n",
    "\n",
    "models_paths = [\n",
    "    # r\"C:\\Users\\Dell\\Desktop\\Bachelor\\image-classification\\misc\\runned\\1800 dataset\\16\\model img_size_16.keras\",\n",
    "    # r\"C:\\Users\\Dell\\Desktop\\Bachelor\\image-classification\\misc\\runned\\1800 dataset\\32\\model img_size_32.keras\",\n",
    "    # r\"C:\\Users\\Dell\\Desktop\\Bachelor\\image-classification\\misc\\runned\\1800 dataset\\64\\model img_size_64.keras\",\n",
    "    # r\"C:\\Users\\Dell\\Desktop\\Bachelor\\image-classification\\misc\\runned\\1800 dataset\\128\\model img_size_128.keras\",\n",
    "    # r\"C:\\Users\\Dell\\Desktop\\Bachelor\\image-classification\\misc\\runned\\1800 dataset\\256\\model img_size_256.keras\",\n",
    "    # r\"C:\\Users\\Dell\\Desktop\\Bachelor\\image-classification\\misc\\runned\\1800 dataset\\300\\model img_size_300.keras\",\n",
    "\n",
    "    # r\"C:\\Users\\Dell\\Desktop\\Bachelor\\image-classification\\misc\\runned\\2500 dataset\\16\\model img_size_16.keras\",\n",
    "    # r\"C:\\Users\\Dell\\Desktop\\Bachelor\\image-classification\\misc\\runned\\2500 dataset\\32\\model img_size_32.keras\",\n",
    "    # r\"C:\\Users\\Dell\\Desktop\\Bachelor\\image-classification\\misc\\runned\\2500 dataset\\64\\model img_size_64.keras\",\n",
    "    # r\"C:\\Users\\Dell\\Desktop\\Bachelor\\image-classification\\misc\\runned\\2500 dataset\\128\\model img_size_128.keras\",\n",
    "    # r\"C:\\Users\\Dell\\Desktop\\Bachelor\\image-classification\\misc\\runned\\2500 dataset\\256\\model img_size_256.keras\",\n",
    "    # r\"C:\\Users\\Dell\\Desktop\\Bachelor\\image-classification\\misc\\runned\\2500 dataset\\300\\model img_size_300.keras\",\n",
    "                r\"C:\\Users\\Dell\\Desktop\\Bachelor\\image-classification\\misc\\runned\\visual clustering\\baa taa thaa\\model_5616_images_img_size_128.keras\"\n",
    "                ]\n",
    "\n",
    "# Load the class labels\n",
    "model_info = r\"C:\\Users\\Dell\\Desktop\\Bachelor\\image-classification\\misc\\runned\\visual clustering\\baa taa thaa\\model_info.json\"\n",
    "with open(model_info, 'r', encoding='utf-8') as f:\n",
    "    # this line was to create the json : json.dump({'index_to_label': index_to_label, 'model_name': model_name}, f, ensure_ascii=False, indent=4)\n",
    "    # we only want the index_to_label\n",
    "    class_labels = json.load(f)['index_to_label']\n",
    "\n",
    "# Open a file chooser and get the image path\n",
    "print(\"Please select an image to classify...\")\n",
    "root = Tk()\n",
    "root.attributes('-topmost', True) # bring the window to the front\n",
    "root.withdraw() # we don't want a full GUI, so keep the root window from appearing\n",
    "root.update()\n",
    "image_path = filedialog.askopenfilename() # show an \"Open\" dialog box and return the path to the selected file\n",
    "# open the image in windows using pillow\n",
    "img = Image.open(image_path)\n",
    "img.show()\n",
    "\n",
    "root.destroy()\n",
    "\n",
    "table_headers = [model_path.split(\"\\\\\")[-3] + \" , \" + model_path.split(\"\\\\\")[-2] for model_path in models_paths]\n",
    "\n",
    "predictions_dict = {label: {} for label in class_labels.values()}\n",
    "printed_text = \"\"\n",
    "\n",
    "# Use the model to predict the class of the image\n",
    "for model_path in models_paths:\n",
    "    trained_data_size = 1800\n",
    "   \n",
    "    with open(model_info, 'r', encoding='utf-8') as f:\n",
    "        img_size = int(json.load(f)['image_size'])\n",
    "    current_model_name = str(trained_data_size) + \" ,\" + str(img_size)\n",
    "    print(\"\\nloading model that was trained on dataset : \", str(trained_data_size) , \", with image size : \", str(img_size) , \" ...\")\n",
    "   \n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    img = image.load_img(image_path, target_size=(img_size, img_size))\n",
    "    # Preprocess the image\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img.astype('float32')/255 \n",
    "\n",
    "    model_prediction = model.predict(img)\n",
    "    highest_probability_class_index = np.argmax(model_prediction)\n",
    "    sorted_predictions = sorted(enumerate(model_prediction[0]), key=lambda x: x[1], reverse=True)    \n",
    "    printed_text += f\"Predictions for model trained on dataset of size {trained_data_size} and image size {img_size}:\\n\"\n",
    "    for i, probability in sorted_predictions:\n",
    "        entity_name = class_labels[str(i)]\n",
    "        predictions_dict[entity_name][current_model_name] = probability\n",
    "        printed_text += f'({class_labels[str(i)]}) :  {probability:.4f}'\n",
    "        if(i == highest_probability_class_index):\n",
    "            printed_text += ' <---------------------------------------------------'   \n",
    "        printed_text += '\\n'\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions_dict).transpose()\n",
    "predictions_table = predictions_df.style.background_gradient(cmap='tab20_r').set_table_styles(\n",
    "    [dict(selector=\"th\", props=[(\"text-align\", \"center\")])]\n",
    ")\n",
    "display(predictions_table)\n",
    "print(printed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Choose a Dataset to test the model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def copy_models_to_kaggle(model_paths):\n",
    "    os.makedirs('/kaggle/working/models', exist_ok=True)\n",
    "    new_paths = []\n",
    "    for model_path in model_paths:\n",
    "        model_trained_data_size = model_path.split(\"/\")[3]\n",
    "        model_trained_img_size = model_path.split(\"/\")[5]\n",
    "        new_model_path = '/kaggle/working/models/' + model_trained_data_size + \"_dataset\" + \"_imgsize_\" + model_trained_img_size + \"_model.keras\"\n",
    "        new_paths.append(new_model_path)\n",
    "        if os.path.exists(new_model_path):\n",
    "            # raise Exception(f\"model already exists at {new_model_path}\")\n",
    "            print(f\"model already exists at {new_model_path}\")\n",
    "        shutil.copy(model_path, new_model_path)\n",
    "    return new_paths\n",
    " \n",
    "\n",
    "batch_size = 32\n",
    "test_dataset_paths = [\n",
    "    '/kaggle/input/sample-1800',\n",
    "    '/kaggle/input/sample-2500',\n",
    "    '/kaggle/input/clean-2500-letters-only',\n",
    "    '/kaggle/input/clean-1800-letters-only'\n",
    "]\n",
    "models_paths = [\n",
    "        '/kaggle/input/1800-model/keras/16/1/model img_size_16.keras',\n",
    "        '/kaggle/input/1800-model/keras/32/1/model img_size_32.keras',\n",
    "        '/kaggle/input/1800-model/keras/64/1/model img_size_64.keras',\n",
    "        '/kaggle/input/1800-model/keras/128/1/model img_size_128.keras',\n",
    "        '/kaggle/input/1800-model/keras/256/1/model img_size_256.keras',\n",
    "        '/kaggle/input/1800-model/keras/300/1/model img_size_300.keras',\n",
    "\n",
    "        '/kaggle/input/2500-model/keras/16/1/model img_size_16.keras',\n",
    "        '/kaggle/input/2500-model/keras/32/1/model img_size_32.keras',\n",
    "        '/kaggle/input/2500-model/keras/64/1/model img_size_64.keras',\n",
    "        '/kaggle/input/2500-model/keras/128/1/model img_size_128.keras',\n",
    "        '/kaggle/input/2500-model/keras/256/1/model img_size_256.keras',\n",
    "        '/kaggle/input/2500-model/keras/300/1/model img_size_300.keras'\n",
    "]\n",
    "print(\"copying models to kaggle to be able to work with WRITE ACESS ...\")\n",
    "copied_models_paths = copy_models_to_kaggle(models_paths)\n",
    "print(f\"loading models in RAM...\")\n",
    "model_path_to_model = {}\n",
    "for model_path in copied_models_paths:\n",
    "    print(f\"loading model in : {model_path} ...\")\n",
    "    model = load_model(model_path)\n",
    "    model_path_to_model[model_path] = model\n",
    "\n",
    "all_datasets_accuracies = {}\n",
    "for dataset_path in test_dataset_paths:\n",
    "    dataset_name  = dataset_path.split(\"/\")[-1]\n",
    "    print(f\"Testing dataset : {dataset_path} ...\")\n",
    "    dataset_test_accuracies = {}\n",
    "    for model_path in copied_models_paths:\n",
    "        # /kaggle/working/models/2500-model_dataset_imgsize_300_model.keras\n",
    "        trained_data_size = int(model_path.split(\"/\")[4].split(\"-\")[0])\n",
    "        print(f\"trained data size : {trained_data_size}\")\n",
    "        img_size = int(model_path.split(\"/\")[4].split(\"_\")[-2])\n",
    "        print(f\"image size : {img_size}\")\n",
    "        current_model_name =  str(trained_data_size) + \" ,\" + str(img_size) + \"_model\"\n",
    "        print(f\"using : {current_model_name} ...\")\n",
    "        model = model_path_to_model[model_path]\n",
    "\n",
    "        print(f\"Testing model : {current_model_name} ...\")\n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        test_generator = test_datagen.flow_from_directory(\n",
    "            dataset_path,\n",
    "            target_size=(img_size, img_size),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            shuffle=False)\n",
    "        print()\n",
    "\n",
    "        # Predict the output\n",
    "        steps = int(np.ceil(test_generator.samples / test_generator.batch_size))\n",
    "        predictions = model.predict(test_generator, steps=steps)\n",
    "        predictions = predictions.argmax(axis=-1)                    \n",
    "        true_classes = test_generator.classes\n",
    "        accuracy = round(accuracy_score(true_classes, predictions), 3)\n",
    "        dataset_test_accuracies[current_model_name] = accuracy\n",
    "        print(f\"Accuracy of model {current_model_name} on dataset {dataset_name} : {accuracy}\")\n",
    "    all_datasets_accuracies[dataset_name] = dataset_test_accuracies\n",
    "\n",
    "print(f\"all datasets accuracies : {all_datasets_accuracies}\")\n",
    "accuracies_df = pd.DataFrame(all_datasets_accuracies).transpose()\n",
    "accuracies_df = accuracies_df.style.background_gradient(cmap='tab20_r').set_table_styles([dict(selector=\"th\", props=[(\"text-align\", \"center\")])])\n",
    "display(accuracies_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Move images from one folder to another</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def move_files_with_rename(src_folder, dst_folder):\n",
    "    # Iterate over all files in the source folder\n",
    "    for filename in os.listdir(src_folder):\n",
    "        src_file = os.path.join(src_folder, filename)\n",
    "        dst_file = os.path.join(dst_folder, filename)\n",
    "\n",
    "        # If a file with the same name exists in the destination folder, rename the file being moved\n",
    "        if os.path.exists(dst_file):\n",
    "            base, extension = os.path.splitext(filename)\n",
    "            filename = f\"{base}_changed{extension}\"\n",
    "            dst_file = os.path.join(dst_folder, filename)\n",
    "\n",
    "        # Move the file\n",
    "        shutil.move(src_file, dst_file)\n",
    "\n",
    "# Use the function\n",
    "src_folder = \"C:/Users/Dell/Desktop/test/500/test\"\n",
    "dst_folder = \"C:/Users/Dell/Desktop/test/500/train\"\n",
    "move_files_with_rename(src_folder, dst_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Divide a dataset into train & test folders </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the source directory and the target directories\n",
    "source_dir = './data set/all/'\n",
    "train_dir = './data set/train/'\n",
    "test_dir = './data set/test/'\n",
    "\n",
    "# Create the target directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Get a list of all the dog and cat image filenames\n",
    "dog_images = [i for i in os.listdir(source_dir) if 'dog' in i]\n",
    "cat_images = [i for i in os.listdir(source_dir) if 'cat' in i]\n",
    "\n",
    "# Split the filenames into training and testing sets\n",
    "train_dogs, test_dogs = train_test_split(dog_images, test_size=0.25)\n",
    "train_cats, test_cats = train_test_split(cat_images, test_size=0.25)\n",
    "\n",
    "# Function to move files\n",
    "def move_files(files, target_dir):\n",
    "    for file in files:\n",
    "        shutil.move(source_dir + file, target_dir + file)\n",
    "\n",
    "# Move the corresponding files into the appropriate directories\n",
    "move_files(train_dogs, train_dir)\n",
    "move_files(test_dogs, test_dir)\n",
    "move_files(train_cats, train_dir)\n",
    "move_files(test_cats, test_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Join folders containing different letter positions into one folder </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "root_dir = r\"C:\\Users\\Dell\\Desktop\\datasets\\1500 (also contains paragraphs)\\1500 (also contains paragraphs)\\isolated_alphabets_per_alphabet\"\n",
    "# dest is beside the root directory\n",
    "dest_dir = os.path.join(os.path.dirname(root_dir), \"joined\")\n",
    "\n",
    "# Get a list of all subdirectories in the root directory\n",
    "subdirs = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "print(\"started\")\n",
    "if(os.path.exists(dest_dir)):\n",
    "    print(\"deleting existing root directory...\")\n",
    "    shutil.rmtree(dest_dir)\n",
    "for subdir in subdirs:\n",
    "    print(\"woring in \", subdir)\n",
    "    if '_' in subdir: # because some subdirectories (numbers) don't have an underscore in their naems and dont need joining\n",
    "        letter_name = subdir.split('_')[0]\n",
    "        new_dir = os.path.join(dest_dir, letter_name)\n",
    "        os.makedirs(new_dir, exist_ok=True)\n",
    "        files = [f for f in os.listdir(os.path.join(root_dir, subdir))]\n",
    "        for file in files:\n",
    "            shutil.copy(os.path.join(root_dir, subdir, file), os.path.join(new_dir, file))\n",
    "    else:\n",
    "        shutil.copytree(os.path.join(root_dir, subdir), os.path.join(dest_dir, subdir))\n",
    "print(\"done\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> same as above code but copy folders only not the files they contain </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "root_dir = r\"C:\\Users\\Dell\\Desktop\\datasets\\1800 - position categorized\\1800 - position categorized\\HMBD-v1-master\\Dataset\"\n",
    "# dest is beside the root directory\n",
    "dest_dir = os.path.join(os.path.dirname(root_dir), \"joined\")\n",
    "\n",
    "# Get a list of all subdirectories in the root directory\n",
    "subdirs = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "print(\"started\")\n",
    "if(os.path.exists(dest_dir)):\n",
    "    print(\"deleting existing root directory...\")\n",
    "    shutil.rmtree(dest_dir)\n",
    "for subdir in subdirs:\n",
    "    print(\"woring in \", subdir)\n",
    "    if '_' in subdir: # because some subdirectories (numbers) don't have an underscore in their naems and dont need joining\n",
    "        letter_name = subdir.split('_')[0]\n",
    "    else :\n",
    "        letter_name = subdir    \n",
    "    new_dir = os.path.join(dest_dir, letter_name)\n",
    "    os.makedirs(new_dir, exist_ok=True)\n",
    "    shutil.copytree(os.path.join(root_dir, subdir), os.path.join(new_dir, subdir))\n",
    "print(\"done\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> move all files from directory to its parent directory , for all subdirectories in the dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <h1> extract all files from directory to its parent directory , for all subdirectories in the dataset </h1>\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "def join_files_in_subdirs(dest,current_dir):\n",
    "    if(not os.path.isdir(current_dir)):\n",
    "        shutil.copy(current_dir, dest)\n",
    "    else:\n",
    "        for subdir in os.listdir(current_dir):\n",
    "            join_files_in_subdirs(dest, os.path.join(current_dir, subdir))\n",
    "        contains_files_only = all(os.path.isfile(os.path.join(current_dir, child_dir)) for child_dir in os.listdir(current_dir))\n",
    "        if contains_files_only:\n",
    "           print (\"done with \", current_dir)\n",
    "           \n",
    "\n",
    "    \n",
    "root_dir = r\"C:\\Users\\Dell\\Desktop\\datasets\\1800 - position categorized\\joined\"\n",
    "dest_dir = os.path.join(os.path.dirname(root_dir), \"joined & merged\")\n",
    "if(os.path.exists(dest_dir)):\n",
    "    print(\"deleting existing root directory...\")\n",
    "    shutil.rmtree(dest_dir)\n",
    "os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "for subdir in os.listdir(root_dir):\n",
    "    merging_dest = os.path.join(dest_dir, subdir)\n",
    "    os.makedirs(merging_dest, exist_ok=True)\n",
    "    join_files_in_subdirs(merging_dest, os.path.join(root_dir, subdir))\n",
    "print(\"done merging \")\n",
    "# open the parent directory of the root directory upon finishing\n",
    "os.startfile(os.path.dirname(dest_dir))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Clear kaggle output folder </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def clear_directory(dir_path):\n",
    "    for filename in os.listdir(dir_path):\n",
    "        file_path = os.path.join(dir_path, filename)\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)  # remove file or symlink\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)  # remove directory\n",
    "\n",
    "# Usage:\n",
    "dir_path = \"/kaggle/working/\"\n",
    "clear_directory(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create a 1 % sample of the dataset in another folder , just to test if the code runs correctly </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "test_dataset_paths = r\"C:\\Users\\Dell\\Desktop\\Bachelor\\datasets\\26 letters\\1800\\1800 letters only\"\n",
    "dest_dir = r\"C:\\Users\\Dell\\Desktop\\Sample_dataset_1800\"\n",
    "\n",
    "# Get a list of all subdirectories in the root directory\n",
    "print(\"started\")\n",
    "if(os.path.exists(dest_dir)):\n",
    "    print(\"deleting existing root directory...\")\n",
    "    shutil.rmtree(dest_dir)\n",
    "subdirs = [d for d in os.listdir(test_dataset_paths)]\n",
    "percent_to_copy = 0.01\n",
    "for subdir in subdirs:\n",
    "    src_entity_dir = os.path.join(test_dataset_paths, subdir)\n",
    "    dest_entity_dir = os.path.join(dest_dir, subdir)\n",
    "    os.makedirs(dest_entity_dir, exist_ok=True)\n",
    "    files = [f for f in os.listdir(src_entity_dir)]\n",
    "    num_files = len(files) \n",
    "    num_files_to_copy = int(num_files * percent_to_copy)\n",
    "    # shuffle the files\n",
    "    random.shuffle(files)\n",
    "    for i in range(num_files_to_copy):\n",
    "        shutil.copy(os.path.join(src_entity_dir, files[i]), dest_entity_dir)\n",
    "    print(\"done with \", subdir , \" , copied only\", num_files_to_copy, \" files\")\n",
    "print(\"done\")\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Merge 2 datasets </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge 2 datasets into one ,\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "first_dataset_path = r\"C:\\Users\\Dell\\Desktop\\Bachelor\\datasets\\26 letters\\1800\\1800 letters only\"\n",
    "second_dataset_path = r\"C:\\Users\\Dell\\Desktop\\Bachelor\\datasets\\26 letters\\2500\\2500 letters only\"\n",
    "dest_dir = r\"C:\\Users\\Dell\\Desktop\\Bachelor\\datasets\\26 letters\\Two datasets merged\"\n",
    "\n",
    "# make a set of all subfodlers names in the first dataset\n",
    "first_dataset_entity_names = set(os.listdir(first_dataset_path))\n",
    "second_dataset_entity_names = set(os.listdir(second_dataset_path))\n",
    "if first_dataset_entity_names != second_dataset_entity_names:\n",
    "    print(\"datasets don't have the same entities\")\n",
    "    print(\"\")\n",
    "\n",
    "for subdir in os.listdir(first_dataset_path):\n",
    "    current_entity_dir = os.path.join(first_dataset_path, subdir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
